"use client";

import { useState, useRef, useEffect } from "react";
import Button from "@/app/components/common/button";

interface RecognitionResult {
    transcript: string;
    timestamp: number;
}

// Web Speech API íƒ€ì… ì •ì˜
interface SpeechRecognitionErrorEvent extends Event {
    error: string;
    message: string;
}

interface SpeechRecognitionEvent extends Event {
    resultIndex: number;
    results: SpeechRecognitionResultList;
}

interface SpeechRecognitionResultList {
    length: number;
    item(index: number): SpeechRecognitionResult;
    [index: number]: SpeechRecognitionResult;
}

interface SpeechRecognitionResult {
    length: number;
    item(index: number): SpeechRecognitionAlternative;
    [index: number]: SpeechRecognitionAlternative;
    isFinal: boolean;
}

interface SpeechRecognitionAlternative {
    transcript: string;
    confidence: number;
}

interface SpeechRecognition extends EventTarget {
    lang: string;
    continuous: boolean;
    interimResults: boolean;
    maxAlternatives: number;
    start(): void;
    stop(): void;
    abort(): void;
    onerror:
        | ((this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any)
        | null;
    onresult:
        | ((this: SpeechRecognition, ev: SpeechRecognitionEvent) => any)
        | null;
    onstart: ((this: SpeechRecognition, ev: Event) => any) | null;
    onend: ((this: SpeechRecognition, ev: Event) => any) | null;
}

interface SpeechRecognitionConstructor {
    new (): SpeechRecognition;
    prototype: SpeechRecognition;
}

declare global {
    interface Window {
        SpeechRecognition: SpeechRecognitionConstructor;
        webkitSpeechRecognition: SpeechRecognitionConstructor;
    }
}

export default function VideoRecorder() {
    const [isRecording, setIsRecording] = useState(false);
    const [transcripts, setTranscripts] = useState<RecognitionResult[]>([]);
    const [currentTranscript, setCurrentTranscript] = useState("");
    const [error, setError] = useState("");

    const videoRef = useRef<HTMLVideoElement>(null);
    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const recognitionRef = useRef<SpeechRecognition | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const chunksRef = useRef<Blob[]>([]);

    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    useEffect(() => {
        return () => {
            stopRecording();
        };
    }, []);

    const startRecording = async () => {
        try {
            setError("");

            // ì›¹ìº  + ë§ˆì´í¬ ì ‘ê·¼
            const stream = await navigator.mediaDevices.getUserMedia({
                video: true,
                audio: true,
            });

            streamRef.current = stream;

            // ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸°
            if (videoRef.current) {
                videoRef.current.srcObject = stream;
            }

            // ë¹„ë””ì˜¤ ë…¹í™” ì‹œì‘
            const mediaRecorder = new MediaRecorder(stream);
            mediaRecorderRef.current = mediaRecorder;
            chunksRef.current = [];

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            mediaRecorder.onstop = () => {
                const blob = new Blob(chunksRef.current, {
                    type: "video/webm",
                });
                // ì—¬ê¸°ì„œ ì„œë²„ì— ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥
                console.log("ë…¹í™” ì™„ë£Œ:", blob);
            };

            mediaRecorder.start();

            // ìŒì„± ì¸ì‹ ì‹œì‘
            if (!window.SpeechRecognition && !window.webkitSpeechRecognition) {
                throw new Error("ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤.");
            }

            const SpeechRecognition =
                window.SpeechRecognition || window.webkitSpeechRecognition;
            const recognition = new SpeechRecognition();

            recognition.lang = "ko-KR";
            recognition.continuous = true; // ê³„ì† ë“£ê¸°
            recognition.interimResults = true; // ì¤‘ê°„ ê²°ê³¼ë„ í‘œì‹œ

            recognition.onresult = (event: SpeechRecognitionEvent) => {
                let interimTranscript = "";
                let finalTranscript = "";

                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }

                if (finalTranscript) {
                    setTranscripts((prev) => [
                        ...prev,
                        {
                            transcript: finalTranscript,
                            timestamp: Date.now(),
                        },
                    ]);
                    setCurrentTranscript("");
                } else {
                    setCurrentTranscript(interimTranscript);
                }
            };

            recognition.onerror = (e: SpeechRecognitionErrorEvent) => {
                console.error("ìŒì„± ì¸ì‹ ì—ëŸ¬:", e);
                setError(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${e.error}`);
            };

            recognition.start();
            recognitionRef.current = recognition;

            setIsRecording(true);
        } catch (err) {
            console.error("ë…¹í™” ì‹œì‘ ì‹¤íŒ¨:", err);
            setError(
                err instanceof Error
                    ? err.message
                    : "ì¹´ë©”ë¼/ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            );
        }
    };

    const stopRecording = () => {
        // ë¹„ë””ì˜¤ ë…¹í™” ì¤‘ì§€
        if (mediaRecorderRef.current) {
            mediaRecorderRef.current.stop();
            mediaRecorderRef.current = null;
        }

        // ìŒì„± ì¸ì‹ ì¤‘ì§€
        if (recognitionRef.current) {
            recognitionRef.current.stop();
            recognitionRef.current = null;
        }

        // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
        if (streamRef.current) {
            streamRef.current.getTracks().forEach((track) => track.stop());
            streamRef.current = null;
        }

        // ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° ì •ë¦¬
        if (videoRef.current) {
            videoRef.current.srcObject = null;
        }

        setIsRecording(false);
        setCurrentTranscript("");
    };

    return (
        <div className="flex min-h-screen flex-col bg-white">
            <div className="flex-1 py-6">
                <h1 className="text-title-small mb-6">
                    ë¹„ë””ì˜¤ ë…¹í™” + ìŒì„± ë³€í™˜
                </h1>

                {/* ë¹„ë””ì˜¤ ë¯¸ë¦¬ë³´ê¸° */}
                <div className="mb-6 overflow-hidden rounded-lg bg-gray-900">
                    <video
                        ref={videoRef}
                        autoPlay
                        playsInline
                        muted
                        className="h-[400px] w-full object-cover"
                    />
                </div>

                {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
                {error && (
                    <div className="mb-4 rounded-lg bg-red-100 p-4 text-red-700">
                        âš ï¸ {error}
                    </div>
                )}

                {/* í˜„ì¬ ì¸ì‹ ì¤‘ì¸ í…ìŠ¤íŠ¸ (ì„ì‹œ) */}
                {currentTranscript && (
                    <div className="mb-4 rounded-lg bg-gray-100 p-4">
                        <p className="text-body5 mb-1 text-gray-500">
                            ì¸ì‹ ì¤‘...
                        </p>
                        <p className="text-body3 text-gray-600">
                            {currentTranscript}
                        </p>
                    </div>
                )}

                {/* ì¸ì‹ëœ í…ìŠ¤íŠ¸ ëª©ë¡ */}
                <div className="mb-4 space-y-2">
                    <h2 className="text-body2 mb-2">ì¸ì‹ëœ í…ìŠ¤íŠ¸</h2>
                    {transcripts.length === 0 ? (
                        <p className="text-body5 text-gray-400">
                            ì•„ì§ ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤
                        </p>
                    ) : (
                        <div className="max-h-[300px] space-y-2 overflow-y-auto">
                            {transcripts.map((item, index) => (
                                <div
                                    key={index}
                                    className="rounded-lg bg-gray-50 p-3"
                                >
                                    <p className="text-body3">
                                        {item.transcript}
                                    </p>
                                    <p className="text-caption mt-1 text-gray-400">
                                        {new Date(
                                            item.timestamp
                                        ).toLocaleTimeString()}
                                    </p>
                                </div>
                            ))}
                        </div>
                    )}
                </div>
            </div>

            {/* ë²„íŠ¼ */}
            <div className="pb-8">
                {!isRecording ? (
                    <Button
                        className="h-[52px] w-full"
                        onClick={startRecording}
                    >
                        ğŸ¥ ë…¹í™” ì‹œì‘
                    </Button>
                ) : (
                    <Button
                        className="h-[52px] w-full"
                        variant="secondary"
                        onClick={stopRecording}
                    >
                        â¹ï¸ ë…¹í™” ì¤‘ì§€
                    </Button>
                )}
            </div>
        </div>
    );
}
